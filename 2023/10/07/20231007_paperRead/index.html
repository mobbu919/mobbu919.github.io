<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/mob.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/mob32.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/mob16.jpg">
  <link rel="mask-icon" href="/images/mob.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"mobbu.space","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":{"gitalk":{"order":-1}},"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="最近在研究多模态视频片段定位方向，在此博客记录一下论文的阅读和我在阅读中的一些个人的理解和想法，之前已经阅读了大部分了，只是因为秋招的原因耽搁了很久，近两年的论文也拉下了。 参考了Awesome-Cross-Modal-Video-Moment-Retrieval整理的论文，后续会添加自己觉得有用的论文。 主要按照年份进行阅读，其中每个年份还会进行分类阅读。中间有详细阅读的论文会添加自己的理解和解">
<meta property="og:type" content="article">
<meta property="og:title" content="多模态视频片段定位论文阅读">
<meta property="og:url" content="http://mobbu.space/2023/10/07/20231007_paperRead/index.html">
<meta property="og:site_name" content="“灵能相谈所”">
<meta property="og:description" content="最近在研究多模态视频片段定位方向，在此博客记录一下论文的阅读和我在阅读中的一些个人的理解和想法，之前已经阅读了大部分了，只是因为秋招的原因耽搁了很久，近两年的论文也拉下了。 参考了Awesome-Cross-Modal-Video-Moment-Retrieval整理的论文，后续会添加自己觉得有用的论文。 主要按照年份进行阅读，其中每个年份还会进行分类阅读。中间有详细阅读的论文会添加自己的理解和解">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-10-07T08:20:38.788Z">
<meta property="article:modified_time" content="2023-10-07T08:45:32.862Z">
<meta property="article:author" content="mobbu">
<meta property="article:tag" content="多模态">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://mobbu.space/2023/10/07/20231007_paperRead/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>多模态视频片段定位论文阅读 | “灵能相谈所”</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">“灵能相谈所”</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://mobbu.space/2023/10/07/20231007_paperRead/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pink_mob.jpeg">
      <meta itemprop="name" content="mobbu">
      <meta itemprop="description" content="道阻且长">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="“灵能相谈所”">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          多模态视频片段定位论文阅读
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-10-07 16:20:38 / 修改时间：16:45:32" itemprop="dateCreated datePublished" datetime="2023-10-07T16:20:38+08:00">2023-10-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/" itemprop="url" rel="index"><span itemprop="name">技术博客</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>20k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>19 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>最近在研究多模态视频片段定位方向，在此博客记录一下论文的阅读和我在阅读中的一些个人的理解和想法，之前已经阅读了大部分了，只是因为秋招的原因耽搁了很久，近两年的论文也拉下了。</p>
<p>参考了<a target="_blank" rel="noopener" href="https://github.com/yawenzeng/Awesome-Cross-Modal-Video-Moment-Retrieval">Awesome-Cross-Modal-Video-Moment-Retrieval</a>整理的论文，后续会添加自己觉得有用的论文。</p>
<p>主要按照年份进行阅读，其中每个年份还会进行分类阅读。中间有详细阅读的论文会添加自己的理解和解析。</p>
<p>持续更新～</p>
<span id="more"></span>

<h1 id="2017-2"><a href="#2017-2" class="headerlink" title="2017:2"></a>2017:2</h1><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/mobbu/p/17746499.html">1[ICCV 2017] [CTRL] TALL Temporal Activity Localization via Language Query</a>⭕<br>2[ICCV 2017] [MCN] Localizing Moments in Video with Natural Language⭕</p>
<p>1[ICCV 2017] [CTRL] TALL Temporal Activity Localization via Language Query</p>
<ul>
<li>动机：开山之作，点出该问题需要解决的两大困难1作为跨模态任务，如何得到适当的文本和视频表示特征，以允许跨模态匹配操作和完成语言查询。2理论上可以生成无限粒度的视频片段，然后逐一比较。但时间消耗过大，那么如何能够从有限粒度的滑动窗口做到准确的具体帧定位。</li>
<li>方法：特征抽取–使用全局的sentence2vec和C3D。模态交互–加+乘+拼+FC。多任务：对齐分数+回归。</li>
</ul>
<p>2[ICCV 2017] [MCN] Localizing Moments in Video with Natural Language</p>
<ul>
<li>方法：特征抽取–使用全局的LSTM和VGG。模态交互–无。多任务：模态间的均方差（而不是预测对齐分数），同时使用RGB和 optical flow光流。</li>
</ul>
<h1 id="2018-5"><a href="#2018-5" class="headerlink" title="2018:5"></a>2018:5</h1><p><a href="">3[MM 2018] Cross-modal Moment Localization in Videos</a></p>
<p>4[SIGIR 2018] Attentive Moment Retrieval in Videos</p>
<p>5[EMNLP 2018] Localizing Moments in Video with Temporal Language</p>
<p>6[EMNLP 2018] Temporally Grounding Natural Sentence in Video</p>
<p>7[IJCAI 2018] Multi-modal Circulant Fusion for Video-to-Language and Backward</p>
<p>[MM 2018] Cross-modal Moment Localization in Videos</p>
<ul>
<li>动机：强调时间语态问题，即句子中的“先”“前”往往被忽视，需要更细致的理解。</li>
<li>方法：尝试结合上下文，用文本给视频加attention。</li>
</ul>
<p>[SIGIR 2018] Attentive Moment Retrieval in Videos</p>
<ul>
<li>方法：尝试结合上下文，用视频给文本加attention。以上两篇的特征处理由于要加attention就开始局部化。</li>
</ul>
<p>[EMNLP 2018] Localizing Moments in Video with Temporal Language</p>
<ul>
<li>动机：时序上下文是很重要的，不然无法理解句子中一些词之间的关系。</li>
<li>方法：延续MCN的思路走算匹配度无模态交互，且同时使用RGB和 optical flow光流。特征抽取–LSTM，视频表示上同时融合了多种上下文。</li>
</ul>
<p>[EMNLP 2018] Temporally Grounding Natural Sentence in Video</p>
<ul>
<li>动机：捕捉视频和句子之间不断发展的细粒度的交互。</li>
<li>方法：从每帧&#x2F;逐词。使用两组lstm处理每个时刻文本和视频的交互。</li>
</ul>
<p>[IJCAI 2018] Multi-modal Circulant Fusion for Video-to-Language and Backward</p>
<ul>
<li>动机：增强视觉和文本表达的融合</li>
<li>方法：同时使用vector和matrix的融合方式，其中circulant matrix的操作方式是每一行平移一个元素以探索不同模态向量的所有可能交互。</li>
</ul>
<p><strong>#年度关键词：上下文。</strong></p>
<h1 id="2019-16"><a href="#2019-16" class="headerlink" title="2019: 16"></a>2019: 16</h1><p>8[AAAI 2019] Localizing Natural Language in Videos<br>9[AAAI 2019] Multilevel language and vision integration for text-to-clip retrieval<br>10[AAAI 2019] Semantic Proposal for Activity Localization in Videos via Sentence Query<br>11[AAAI 2019] To Find Where You Talk Temporal Sentence Localization in Video with Attention Based Location Regression<br>12[AAAI 2019] Read, Watch, and Move Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos⭕<br>13[CVPR 2019] MAN_ Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment⭕<br>14[CVPR Workshop 2019] Tripping through time Efficient Localization of Activities in Videos<br>15[CVPR 2019] Language-Driven Temporal Activity Localization A Semantic Matching Reinforcement Learning Model⭕<br>16[CVPR 2019] Weakly Supervised Video Moment Retrieval From Text Queries⭕<br>17[ICMR 2019] Cross-Modal Video Moment Retrieval with Spatial and Language-Temporal Attention⭕<br>18[MM 2019] Exploiting Temporal Relationships in Video Moment Localization with Natural Language<br>19[NIPS 2019] Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos<br>20[SIGIR 2019] Cross-modal interaction networks for query-based moment retrieval in videos<br>21[WACV2019] MAC: Mining Activity Concepts for Language-Based Temporal Localization<br>22[ArXiv 2019] Temporal Localization of Moments in Video Collections with Natural Language<br>23[ArXiv 2019] wMAN Weakly-supervised Moment Alignment Network for Text-based Video Segment Retrieval  </p>
<p>[AAAI 2019] Localizing Natural Language in Videos</p>
<ul>
<li>动机：以前的工作忽略模态间细粒度的交互和上下文信息。</li>
<li>方法：交叉门控的递归网络来匹配自然句子和视频序列（类似cross-attention），以完成细粒度交互。</li>
</ul>
<p>[AAAI 2019] Multilevel language and vision integration for text-to-clip retrieval</p>
<ul>
<li>动机：global embedding缺少细粒度，且不同模态之间独立嵌入缺乏交互。</li>
<li>方法：先融语义再用R-C3D生成候选时刻以加快效率。同时用LSTM对查询语句和视频片段之间的细粒度相似性进行时序融合。</li>
</ul>
<p>[AAAI 2019] Semantic Proposal for Activity Localization in Videos via Sentence Query</p>
<ul>
<li>动机：滑动窗口数据量多，效率低。</li>
<li>方法：将句子中的语义信息集成到动作候选生成过程中以得到更好的时刻候选集。</li>
<li>其他：该任务不同于时序动作定位的地方 （1）某一个动作是各种行动的组合，可能持续很长时间。 （2）句子查询不限于预定义的类列表。 （3）视频通常包含多个不同的活动实例。</li>
</ul>
<p>[AAAI 2019] To Find Where You Talk Temporal Sentence Localization in Video with Attention Based Location Regression</p>
<ul>
<li>动机：1视频的时间结构和全局上下文没有充分探索2句子处理太粗3候选切分太耗时。</li>
<li>方法：特征抽取–全局+局部，即用C3D+BiLSTM处理视频，用Glove+BiLSTM处理文本。然后co-attention进行对齐。并且不做切分，直接预测定位点。</li>
</ul>
<p>[AAAI 2019] Read, Watch, and Move Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos</p>
<ul>
<li>动机：候选太耗时。</li>
<li>方法：首次将强化学习引入到这个领域，处理成顺序决策问题。</li>
</ul>
<p>[CVPR 2019] MAN_ Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment</p>
<ul>
<li>动机：语义失调和结构失调（候选集之间的处理是独立的）。</li>
<li>方法：特征抽取–LSTM和I3D。使用迭代图学习候选时刻之间的关系。</li>
</ul>
<p>[CVPR Workshop 2019] Tripping through time Efficient Localization of Activities in Videos</p>
<ul>
<li>动机：1文本和视频的联合表示以local细粒化。2候选效率低且不符合人类的自然定位方法。</li>
<li>方法：使用门控注意力建模细粒度的文本和视觉表示+强化学习定位。</li>
</ul>
<p>[CVPR 2019] Language-Driven Temporal Activity Localization A Semantic Matching Reinforcement Learning Model</p>
<ul>
<li>动机：直接匹配句子与视频内容使强化学习表现不佳，更需要更多的关注句子和图片中目标的细粒度来增强语义。</li>
<li>方法：在强化学习框架下引入语义特征。</li>
</ul>
<p>[CVPR 2019] Weakly Supervised Video Moment Retrieval From Text Queries</p>
<ul>
<li>动机：标注句子边界太耗时，且不可扩展在实践中</li>
<li>方法：弱监督–在训练时不需要对文本描述进行时间边界注释，而只使用视频级别的文本描述。技术上主要使用文本和视频在时序上的注意力对齐。</li>
</ul>
<p>[ICMR 2019] Cross-Modal Video Moment Retrieval with Spatial and Language-Temporal Attention</p>
<ul>
<li>动机：没有关注空间目标信息和文本的语义对齐，且需要结合视频的时空信息。</li>
<li>方法：特征抽取–全局+局部，即全局还是用C3D和BiLSTM。然后全局特征分别给局部（词和目标）加attention突出重点。</li>
</ul>
<p>[MM 2019] Exploiting Temporal Relationships in Video Moment Localization with Natural Language</p>
<ul>
<li>动机：之前对文本的处理是Global，而文本中事件之间的时间依赖和推理没有得到充分考虑。</li>
<li>方法：树注意网络将文本分为主事件，上下文时间，时间信号。</li>
</ul>
<p>[NIPS 2019] Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos</p>
<ul>
<li>动机：现有的方法主要做语义匹配和对齐，而忽略了句子信息在视频的时间关联和组合中起着重要作用。</li>
<li>方法：提出语义条件动态调制(SCDM)机制，它依靠句子语义来调节时间卷积运算，从而更好地关联和组合句子相关的视频内容。</li>
</ul>
<p>[SIGIR 2019] Cross-modal interaction networks for query-based moment retrieval in videos</p>
<ul>
<li>动机：对句子处理的细粒度不够（语境长依赖）。</li>
<li>方法：对切分的词构建语义树&#x2F;图抽特征。</li>
</ul>
<p>[WACV2019] MAC: Mining Activity Concepts for Language-Based Temporal Localization</p>
<ul>
<li>动机：联合嵌入的子空间忽略了有关视频和查询中动作的丰富语义线索</li>
<li>方法：从视频和语言模态中挖掘动作概念，其他部分同TALL。</li>
</ul>
<p>[ArXiv 2019] Temporal Localization of Moments in Video Collections with Natural Language</p>
<ul>
<li>动机：欧几里德居来来衡量文本视频距离不好（MCN的对比路线），无法大规模应用。</li>
<li>方法：使用平方差距离更精准的对齐，并应用视频重排序方法以应对大规模视频库。</li>
</ul>
<p>[ArXiv 2019] wMAN Weakly-supervised Moment Alignment Network for Text-based Video Segment Retrieval</p>
<ul>
<li>动机：自动推断无监督的视觉和语言表示之间的潜在对应关系&#x2F;对齐。</li>
<li>方法：词条件视觉图以捕捉交互，并应用多实例学习完成无监督下的细粒度对齐。</li>
</ul>
<p><strong>#年度关键词：文本和视频的细粒化+强化学习+候选框关系&#x2F;生成+弱监督。</strong></p>
<h1 id="2020-22"><a href="#2020-22" class="headerlink" title="2020:22"></a>2020:22</h1><p>24[AAAI 2020] Temporally Grounding Language Queries in Videos by Contextual Boundary-aware Prediction<br>25[AAAI 2020] Tree-Structured Policy based Progressive Reinforcement Learning for Temporally Language Grounding in Video<br>26[AAAI 2020] Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language⭕<br>27[AAAI 2020] Weakly-Supervised Video Moment Retrieval via Semantic Completion Network<br>28[TIP 2020] Moment Retrieval via Cross-Modal Interaction Networks With Query Reconstruction⭕<br>29[MM 2020] Adversarial Video Moment Retrieval by Jointly Modeling Ranking and Localization⭕<br>30[MM 2020] STRONG: Spatio-Temporal Reinforcement Learning for Cross-Modal Video Moment Localization<br>31[MM 2020] Dual Path Interaction Network for video Moment Localization<br>32[MM 2020] Regularized Two-Branch Proposal Networks for Weakly-Supervised Moment Retrieval in Videos<br>33[ECCV 2020] VLANet: Video-Language Alignment Network for Weakly-Supervised Video Moment Retrieval<br>34[WACV2020] Proposal-free Temporal Moment Localization of a Natural-Language Query in Video using Guided Attention<br>35[ECCV 2020] TVR A Large-Scale Dataset for Video-Subtitle Moment Retrieval<br>36[ArXiv 2020] Video Moment Localization using Object Evidence and Reverse Captioning<br>37[ArXiv 2020] Graph Neural Network for Video-Query based Video Moment Retrieval<br>38[ArXiv 2020] Text-based Localization of Moments in a Video Corpus<br>39[ArXiv 2020] Generating Adjacency Matrix for Video-Query based Video Moment Retrieval<br>40[BMVC 2020] Uncovering Hidden Challenges in Query-Based Video Moment Retrieval<br>41[ArXiv 2020] Video Moment Retrieval via Natural Language Queries<br>42[ArXiv 2020] Frame-wise Cross-modal Match for Video Moment Retrieval<br>43[MM 2020] Fine-grained Iterative Attention Network for Temporal Language Localization in Videos<br>44[MM 2020] Jointly Cross- and Self-Modal Graph Attention Network for Query-Based Moment Localization<br>45[CVPR 2020] Dense Regression Network for Video Grounding  </p>
<p>[AAAI 2020] Temporally Grounding Language Queries in Videos by Contextual Boundary-aware Prediction</p>
<ul>
<li>动机：当模型无法定位最佳时刻时，添加的偏移回归可能会失败。</li>
<li>方法：语言集成的“感知”周围的预测，即每个时间步长都会预测时间锚和边界。</li>
</ul>
<p>[AAAI 2020] Tree-Structured Policy based Progressive Reinforcement Learning for Temporally Language Grounding in Video</p>
<ul>
<li>动机：现有的方法大多存在效率低下、缺乏可解释性和偏离人类感知机制（从粗到细）问题。</li>
<li>方法：基于树结构政策的渐进强化学习(TSP-PRL)框架，通过迭代细化过程来顺序地调节时间边界。</li>
</ul>
<p>[AAAI 2020] Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language</p>
<ul>
<li>动机：以往模型一般单独考虑时间，忽视了时间的依赖性。（不能在当前时刻对其他时刻进行预测）。</li>
<li>方法：时间从一维变为二维，即变成二维矩阵，预测每一个位置的score。</li>
</ul>
<p>[AAAI 2020] Weakly-Supervised Video Moment Retrieval via Semantic Completion Network</p>
<ul>
<li>动机：弱监督的注意力权重通常只对很明显的区域有反应，即候选太相似。</li>
<li>方法：候选生成–视觉语义词和文本对齐得到分数，并反馈给候选生成的打分。</li>
</ul>
<p>[TIP 2020] Moment Retrieval via Cross-Modal Interaction Networks With Query Reconstruction</p>
<ul>
<li>sigir19的扩充，增加查询重建任务，整体上联合自然语言查询的句法依赖、视频上下文中的长程语义依赖和足够的跨模态交互一起建模。</li>
</ul>
<p>[MM 2020] Adversarial Video Moment Retrieval by Jointly Modeling Ranking and Localization</p>
<ul>
<li>动机：强化学习定位不稳定，检索任务候选框效率低。</li>
<li>方法：用对抗学习联合建模，即用强化学习定位来生成候选，进而在生成候选集上进行检索。</li>
</ul>
<p>[MM 2020] STRONG: Spatio-Temporal Reinforcement Learning for Cross-Modal Video Moment Localization</p>
<ul>
<li>动机：空间上的场景信息很重要+候选框效率低。</li>
<li>方法：除了时序上的强化学习，其在空间上也加入强化学习，并以弱监督追踪的方式聚焦场景，去除冗余。</li>
</ul>
<p>[MM 2020] Dual Path Interaction Network for video Moment Localization</p>
<ul>
<li>动机：很少有研究将候选框层级的对齐信息（算语义匹配分数的技术路线）和帧层级的边界信息（预测边界起止的路线）结合在一起，并考虑它们之间的互补性。</li>
<li>方法：双路径交互网络(DPIN)，分别做两大任务，同时两者之间通过信息交互相互增强。</li>
</ul>
<p>[MM 2020] Regularized Two-Branch Proposal Networks for Weakly-Supervised Moment Retrieval in Videos</p>
<ul>
<li>动机：大多数现有的弱监督方法都采用基于多实例的框架来做样本间对抗，但忽略了相似内容之间的样本间对抗。</li>
<li>方法：同时考虑样本间和样本内的对抗，其中会采用语言感知得到两个分支的视觉表示（被增强的和被印制的），这两者将构成很强的正负样本对。</li>
</ul>
<p>[ECCV 2020] VLANet: Video-Language Alignment Network for Weakly-Supervised Video Moment Retrieval</p>
<ul>
<li>动机：粗略的查询表示和单向注意机制导致模糊的注意映射。</li>
<li>方法：修剪候选集+更细粒度的多注意力机制来获得更好的注意力增加定位性能。</li>
</ul>
<p>[WACV2020] Proposal-free Temporal Moment Localization of a Natural-Language Query in Video using Guided Attention</p>
<ul>
<li>动机：不依赖候选的端对端框架，文本注释主观性。</li>
<li>方法：使用attention的动态过滤器（利用文本对视频的过滤），软标签分布损失解决文本注释的不确定性。</li>
</ul>
<p>[ECCV 2020] TVR A Large-Scale Dataset for Video-Subtitle Moment Retrieval</p>
<ul>
<li>动机：视频+字幕才能进一步的理解。</li>
<li>方法：提出一个新数据集，并设计一个类似多Transformer的模型完成不同模态的理解和融合。</li>
</ul>
<p>[ArXiv 2020] Video Moment Localization using Object Evidence and Reverse Captioning</p>
<ul>
<li>动机：仅仅有语义概念是不够的，还需要对象和字幕的补充。</li>
<li>方法：在MAC（视频，视觉语义概念，文本，文本语义概念）的方法上再增加对象和字幕的特征。</li>
</ul>
<p>[ArXiv 2020] Graph Neural Network for Video-Query based Video Moment Retrieval</p>
<ul>
<li>动机：帧间的特征相似度与视频间的特征相似度之间存在不一致，这会影响特征融合。</li>
<li>方法：沿时间维度对视频构多个图，然后再做多图特征融合。</li>
</ul>
<p>[ArXiv 2020] Text-based Localization of Moments in a Video Corpus</p>
<ul>
<li>动机：text-video可能会有一对多+搜索全库是必要的</li>
<li>方法：层次时刻对齐网络能更好的编码跨模态特征以理解细微的差异，然后做模态内的三元组和模态间的三元组以应对搜索所有的时刻。</li>
</ul>
<p>[ArXiv 2020] Generating Adjacency Matrix for Video-Query based Video Moment Retrieval</p>
<ul>
<li>动机：现有利用图技术的方法得到的邻接矩阵是固定的。</li>
<li>方法：使用图抽取模态内和模态间的特征关系。</li>
</ul>
<p>[BMVC 2020] Uncovering Hidden Challenges in Query-Based Video Moment Retrieval</p>
<ul>
<li>讨论性文章：虽然现有技术已经取得了很好的提高，这篇文章探究了存在的两大问题，1数据集存在偏差，2评价指标不一定可靠。</li>
</ul>
<p>[ArXiv 2020] Video Moment Retrieval via Natural Language Queries</p>
<ul>
<li>动机：起止时间标记有噪音，需要结合上下文的更好表示。</li>
<li>方法：多头自我注意+交叉注意力，以捕获视频&#x2F;查询交互和远程查询依存关系。多任务学习中加入moment segmentation任务。</li>
</ul>
<p>[ArXiv 2020] Frame-wise Cross-modal Match for Video Moment Retrieval</p>
<ul>
<li>动机：以往的方法不能很好地捕获查询和视频帧之间的跨模态交互。</li>
<li>方法：利用注意力对齐，做帧间的预测和边界预测两个任务。</li>
</ul>
<p>[MM 2020] Fine-grained Iterative Attention Network for Temporal Language Localization in Videos</p>
<ul>
<li>动机：现有工作只集中在从视频到查询的单向交互上。</li>
<li>方法：针对句子和视频的cross-attention做双向相互。</li>
</ul>
<p>[MM 2020] Jointly Cross- and Self-Modal Graph Attention Network for Query-Based Moment Localization</p>
<ul>
<li>动机：视频大部分都无关，只有小部分视频相关，所以应该更加重视两者的融合。</li>
<li>方法：提出模态间和模态内两种交互模式，以帧和词构图通过消息传递来增强表示。</li>
</ul>
<p>[CVPR 2020] Dense Regression Network for Video Grounding</p>
<ul>
<li>动机：以往训练的模型都是在正负例样本不平衡的情况下，所以作者尝试利用帧之间的距离作为更密集的监督信号。</li>
<li>方法：预测起始结束帧，然后认为只要在ground truth中的片段都认为是正例。</li>
</ul>
<p><strong>#年度关键词：任务扩展（如对全库搜索moment，增加字幕，标定鲁棒性等等），继续细化和融合，候选框关系。</strong><br>#研究热情高涨，水文趋势开始增加….</p>
<h1 id="2021-11"><a href="#2021-11" class="headerlink" title="2021:11"></a>2021:11</h1><p>46[CVPR 2021] Multi-Modal Relational Graph for Cross-Modal Video Moment Retrieval⭕<br>47[WACV2021] LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video Moment Retrieval<br>48[arxiv2021] A Closer Look at Temporal Sentence Grounding in Videos_ Datasets and Metrics⭕<br>49[CVPR2021] Interventional Video Grounding with Dual Contrastive Learning<br>50[SIGIR2021] Video Corpus Moment Retrieval with Contrastive Learning<br>51[SIGIR2021] Deconfounded Video Moment Retrieval with Causal Intervention<br>52[SIGIR2021] Cross Interaction Network for Natural Language Guided VideoMoment Retrieval<br>53[ACL2021] mTVR: Multilingual Moment Retrieval in Videos<br>53[ICCV2021] Fast Video Moment Retrieval<br>54[MM2021] CONQUER: Contextual Query-aware Ranking for Video Corpus Moment Retrieval  </p>
<p>[CVPR 2021] Multi-Modal Relational Graph for Cross-Modal Video Moment Retrieval</p>
<ul>
<li>动机：基于候选的方法会导致候选之间非常相似难以区别，那么直接理解内容（局部目标）的细微变化会是更好的选择。</li>
<li>方法：提出多模态关系图模型来捕捉视频中的局部目标之间的时空关系，并且在图预训练的框架下优化特征表达。</li>
</ul>
<p>[WACV2021] LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video Moment Retrieval</p>
<ul>
<li>动机：改善视频与文本之间的 latent alignment，学习上下文的视觉语义表征。</li>
<li>方法：提出 latent co-attention 模型，通过多级 coattention 机制+在视频上的positional encodings。</li>
</ul>
<p>[Arxiv2021] A Closer Look at Temporal Sentence Grounding in Videos_ Datasets and Metrics</p>
<ul>
<li>动机：这应该是最近很不错的文章了，详细讨论了现有数据集的偏差问题（即可能模型不需要学什么内容，直接学数据偏差就有不错的结果）。</li>
<li>方法：然后作者提供了新的datasets和metrics，这个工作其实展现了现有的经典模型还是有比较多的问题，在这种去偏数据集下表现得很不理想。</li>
</ul>
<p>[CVPR2021] Interventional Video Grounding with Dual Contrastive Learning</p>
<ul>
<li>动机：这篇同样也是探究数据集中的选择偏差，从而使某些text和moment存在虚假关联。</li>
<li>方法：提出双对比学习+因果推理。双对比学习是VV和QV，即尝试尽可能分离moment之间和moment query之间的语义。因果推理则利用因果干预P(Y|do(X))和事件作为混杂因素来学习表示。</li>
</ul>
<p>[SIGIR2021] Video Corpus Moment Retrieval with Contrastive Learning</p>
<ul>
<li>动机：一般使用的跨模态交互学习，高效率和高质量检索之间很难同时保持。</li>
<li>方法：直接引入对比学习来改进视频编码器和文本编码器。其中视频对比学习旨在最大程度地提高查询和候选视频之间的相互信息。帧对比学习的目的是突出视频中帧级查询对应的片段。</li>
</ul>
<p>[SIGIR2021] Deconfounded Video Moment Retrieval with Causal Intervention</p>
<ul>
<li>动机：现有的模型往往都利用数据集偏差，如何减少这种偏差是很重要的。</li>
<li>方法：提出一个结构性受因果关系启发的框架。具体来说提出解纠缠的跨模态匹配(DCM)方法来消除位置对预测结果的混杂效应，其先推断视觉内容的核心特征，然后用对基于后门调整的解纠缠多模态输入进行因果干预，迫使模型公平地考虑目标的每个可能的位置。</li>
</ul>
<p>[SIGIR2021] Cross Interaction Network for Natural Language Guided Video Moment Retrieval</p>
<ul>
<li>动机：现有技术一般只强调文本到视频的单交互，且设计复杂效率低。</li>
<li>方法：提出结合一种自我注意和交叉交互的多头部注意机制，以捕获视频查询的内部依赖关系以及两个方向的相互关系。</li>
</ul>
<p>[ACL2021] mTVR: Multilingual Moment Retrieval in Videos</p>
<ul>
<li>动机：短文。提出一个的大规模新数据集，其中有双语数据。</li>
</ul>
<p>[ICCV2021] Fast Video Moment Retrieval</p>
<ul>
<li>动机：现有模型的跨模态交互模块太耗时，想要提高时间。</li>
<li>方法：提出细粒度的语义蒸馏框架来转移知识，即用跨模态公共空间替换跨模态交互模块，其中蒸馏过程将语义转移到公共空间。</li>
</ul>
<p>[MM2021] CONQUER: Contextual Query-aware Ranking for Video Corpus Moment Retrieval</p>
<ul>
<li>动机：目前的单阶段方法无法处理实时的结果，同时计算和存储复杂。</li>
<li>方法：使用两阶段的步骤，即这两个步骤是先依赖查询进行视频内容的自适应融合，再执行双向注意以结合某个clip内容进行时刻定位学习，以学习聚合剪辑和查询信息的新表示。</li>
</ul>
<p>[MM2021] Visual Co-Occurrence Alignment Learning for Weakly-Supervised Video Moment Retrieval</p>
<p><strong>#年度关键词：数据集问题，更细致的视频理解，对比学习。</strong><br>#注：由于文章众多，暂时只整理顶会中比较角度不一样的文章。</p>
<h1 id="2022-19"><a href="#2022-19" class="headerlink" title="2022:19"></a>2022:19</h1><p>55[Survey] A Survey on Temporal Sentence Grounding in Videos⭕<br>56[Survey] The Elements of Temporal Sentence Grounding in Videos: A Survey and Future Directions⭕<br>57[Book]Towards temporal sentence grounding in videos<br>57[TIP2022] Video Moment Retrieval With Cross-Modal Neural Architecture Search<br>58[TMM2022] Regularized Two Granularity Loss Function for Weakly Supervised Video Moment Retrieval<br>59[TOMM2022] Moment is Important: Language-Based Video Moment Retrieval via Adversarial Learning<br>60[CVPR2022] UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection<br>61[CVPR2022] AxIoU: An Axiomatically Justified Measure for Video Moment Retrieval<br>62[Arxiv2022] Explore and Match: End-to-End Video Grounding with Transformer<br>63[AAAI2022] Unsupervised Temporal Video Grounding with Deep Semantic Clustering⭕<br>64[AAAI2022] Memory-Guided Semantic Learning Network for Temporal Sentence Grounding<br>65[ICMR2022] Learning Sample Importance for Cross-Scenario Video Temporal Grounding<br>66[SIGIR2022] Point Prompt Tuning for Temporally Language Grounding⭕<br>67[SIGIR2022] Video Moment Retrieval from text Queries via Single Frame Annotation<br>68[SIGIR2022] You Need to Read Again: Multi-granularity Perception Network for Moment Retrieval in Videos<br>69[ECCV2022] Selective Query-Guided Debiasing for Video Corpus Moment Retrieval<br>70[EMNLP2022] Modal-specific Pseudo Query Generation for Video Corpus Moment Retrieval<br>71[MM2022] Interactive Video Corpus Moment Retrieval using Reinforcement Learning<br>72[MM2022] Prompt-based Zero-shot Video Moment Retrieval<br>73[MM2022] Video Moment Retrieval with Hierarchical Contrastive Learning  </p>
<p>[Survey] A Survey on Temporal Sentence Grounding in Videos<br>[Survey] The Elements of Temporal Sentence Grounding in Videos: A Survey and Future Directions<br>[Book]Towards temporal sentence grounding in videos  </p>
<p>动机：这几篇综述文章都写的很好，可以详细看看<br>[TIP2022] Video Moment Retrieval With Cross-Modal Neural Architecture Search</p>
<p>动机：以往的工作都依赖手工or专家设计，不够灵活。<br>方法：使用神经架构搜索来做VMR任务。具体来说，基于有向无环图搜索可重复的单元网络架构，然后，通过查询感知的注意力调节无环图中的边权。<br>[TMM2022] Regularized Two Granularity Loss Function for Weakly Supervised Video Moment Retrieval</p>
<p>动机：弱监督视频时刻检索。<br>方法：设计双粒度损失函数考虑视频级和实例级关系。具体来说，先生成粗粒度的视频片段，然后利用所有视频内片段（即正例）和文本描述之间的多实例学习。然后将此过程视为噪声标签下的弱监督学习任务来对进一步这些片段进行分类进行精细度定位。<br>[TOMM2022] Moment is Important: Language-Based Video Moment Retrieval via Adversarial Learning</p>
<p>动机：[29]的扩展论文，对抗性VMR在训练时容易不稳定。<br>方法：因此将多个子任务变为持续学习的形式以优化对抗性VMR的学习。<br>[CVPR2022] UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection</p>
<p>动机：联合moment retrieval和highlight detection这俩子任务以更好地为应用服务。<br>方法：视频和音频首先通过uni-modal进行融合，如果query不提供将生成query，不然就直接完成编码后预测highlight和offset。<br>[CVPR2022] AxIoU: An Axiomatically Justified Measure for Video Moment Retrieval</p>
<p>动机：现在评价这个领域的性能，一般只使用R@K，但它的缺点在于1对排名不敏感；2使用二值化的IoU会损失细粒度质量，因此作者们提出了一个新指标AxIoU。<br>方法：AxIoU是平均最大IoU，其满足对最佳片段的冗余不变性和单调性，值得一看。<br>[Arxiv2022] Explore and Match: End-to-End Video Grounding with Transformer</p>
<p>动机：统一proposal-free（直接预测的探索）和proposal-based（切分再检索的匹配）这俩方案。<br>方法：将该任务定义为一个集合预测问题，先探索再匹配。首先时间定位损失执行proposal-free，然后设置指导损失将每个查询进行匹配。<br>[AAAI2022] Unsupervised Temporal Video Grounding with Deep Semantic Clustering</p>
<p>动机：无监督TVG。在现实场景中收集数据既昂贵又耗时，这是第一篇做无监督的文章。<br>方法：使用提出深度语义聚类网络（DSCNet）利用整个查询集中的所有语义信息来组合每个视频中可能的活动以进行定位。具体来说，先从整个查询集中提取隐式语义特征，然后将其作为指导在视频中组合活动，最后利用前景注意力分支过滤掉多余的背景活动并细化定位结果。<br>[AAAI2022] Memory-Guided Semantic Learning Network for Temporal Sentence Grounding</p>
<p>动机：数据分布不平衡，容易忘记训练过程中很少出现的情况，这会影响模型的泛化能力。<br>方法：使用记忆增强网络来学习和记忆。具体来说，跨模态图卷积网络对齐给定的视频查询对，然后利用内存模块将跨模态共享语义特征记录内存中，来减轻遗忘问题。<br>[ICMR2022] Learning Sample Importance for Cross-Scenario Video Temporal Grounding</p>
<p>动机：针对superficial biases的文章（如[48]中提过的某些时间偏好等等）。<br>方法：提出去偏时间语言定位器 (DebiasTLL) ，其同时训练两个模型，如果这两个模型在判断样本时的预测差异大，则成为有偏样本的可能性更高。因此利用信息差异设计数据重新加权方案来减轻数据偏差。<br>[SIGIR2022] Point Prompt Tuning for Temporally Language Grounding</p>
<p>动机：基于Prompt learning的新方法。作者认为现有方法都没有能够利用好现有预训练大模型的成果，但其实利用Prompt技巧就能够做得很好。<br>方法：加入Prompt、改装TLG任务以适应大模型，几乎是文本和视频直接作为input输入即可。速度较快，训练只需要训练几层FC，就能轻松利用大模型的语义能力。<br>[SIGIR2022] Video Moment Retrieval from text Queries via Single Frame Annotation</p>
<p>动机：完全监督很贵，但其实只需要”glance annotation”就够了。<br>方法：只需要在完全监督内的随机帧，即“一瞥”，加上对比学习在clip和query之间进行对比，其中“一瞥”的高斯分布权重分配给所有clip。<br>[SIGIR2022] You Need to Read Again: Multi-granularity Perception Network for Moment Retrieval in Videos</p>
<p>动机：以前的方法倾向于以粗略的方式执行单模态学习和跨模态交互，而忽略了视频内容、查询上下文及其对齐中包含的细粒度线索。<br>方法：提出多粒度感知网络，在多粒度级别感知模态内和模态间信息。<br>[ECCV2022] Selective Query-Guided Debiasing for Video Corpus Moment Retrieval</p>
<p>动机：一篇矫正定位偏差的工作。作者们认为现有模型都是在学习查询和片段之间的共现pattern，从而使模型将查询中对象（例如铅笔）与该对象经常出现在视频中的片段（例如用铅笔书写的场景）虚假关联。<br>方法：提出Selective Query-guided Debiasing network (SQuiDNet)，来进行selective debiasing。<br>[EMNLP2022] Modal-specific Pseudo Query Generation for Video Corpus Moment Retrieval</p>
<p>动机：目前的方法们强依赖于数据集在时序上的昂贵注释，因此提出一个自监督方法来缓解。<br>方法：自监督学习框架即，Modal-specific Pseudo Query Generation（MPGN）。具体来说，MPGN通过字幕的时刻采样来获取时间的时刻，然后用所选时间时刻的视觉和文本信息生成伪查询。<br>[MM2022] Interactive Video Corpus Moment Retrieval using Reinforcement Learning</p>
<p>动机：目前的方法们对于相似的片段、或候选再很深或隐秘的时候，模型需要长时间的浏览和结果检查，因此文章基于强化学习期望在几轮交互内就达到搜索目标。<br>方法：具体来说，系统根据反馈交互式地规划导航路径，并推荐一个潜在目标，最大限度地提高用户评论的长期奖励。<br>[MM2022] Prompt-based Zero-shot Video Moment Retrieval</p>
<p>动机：作者们认为由于视频和文本学习过程的分离，全局视觉特征往往被忽略。因此作者提出了一种基于提示的零样本视频时刻检索（PZVMR）方法。<br>方法：这种基于提示学习的方法分为提案提示（PP）和动词提示（VP），以使得模型从预先训练的 CLIP 中提取的任务相关知识，并将这些知识适应这个任务。<br>[MM2022] Video Moment Retrieval with Hierarchical Contrastive Learning</p>
<p>动机：目前的方法们大多只关注对齐查询和单级剪辑或时刻特征，而忽略了视频本身涉及的不同粒度。<br>方法：提出了一种具有分层对比学习的时间定位网络（HCLNet）。具体来说，作者们引入了一种分层对比学习方法，通过最大化查询和三种不同粒度的视频之间的互信息（MI）来更好地对齐查询和视频，以学习信息表示。<br>#年度关键词：新的任务形态、多范式的结合、更高效、提示学习、去偏和新指标。<br>#新玩法开始变得多样起来。</p>
<h1 id="2023-x"><a href="#2023-x" class="headerlink" title="2023:x"></a>2023:x</h1><p>74[Survey] Temporal Sentence Grounding in Videos: A Survey and Future Directions<br>75[Survey] A Closer Look at Debiased Temporal Sentence Grounding in Videos: Dataset, Metric, and Approach<br>76[TMM2023] Temporally Language Grounding with Multi-modal Multi-Prompt Tuning<br>77[ACL2023] CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding<br>78[ACL2023] Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding<br>79[SIGIR2023] RewardTLG: Learning to Temporally Language Grounding from Flexible Reward  </p>
<p>[Survey] Temporal Sentence Grounding in Videos: A Survey and Future Directions<br>[Survey] A Closer Look at Debiased Temporal Sentence Grounding in Videos: Dataset, Metric, and Approach  </p>
<p>动机：综述文章越来越多<br>[TMM2023] Temporally Language Grounding with Multi-modal Multi-Prompt Tuning</p>
<p>动机：多模态预训练+提示学习。<br>方法：这篇文章讨论了各种提示模版对于这一任务的影响，包括固定prompt和可学习的prompt。<br>[ACL2023] CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding</p>
<p>动机：对于定位任务来说，长视频的需求更大，但目前探索少，主要是由于长视频的推理速度。<br>方法：为了提速，作者提出一种高效的从粗到细的对齐框架，主要是查询引导的窗口选择策略来加速推理+结合对比学习从粗到细，在Ego4D-NLQ 上的推理时间加快2倍，MAD上的推理时间快15倍。<br>[ACL2023] Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding</p>
<p>动机：基于Pre-Trained Language Models，并评估了大模型中参数高效训练的适用性。<br>方法：做了很多有意义的实验，其中1更大的语言模型肯定是对效果有增益的；2adapter训练而不是全参数对于大模型来说更划算。<br>[SIGIR2023] RewardTLG: Learning to Temporally Language Grounding from Flexible Reward</p>
<p>动机：大模型之后，reward model也来了。<br>方法：这篇文章收集了人类反馈数据训了一个奖励模型，用于指导强化学习的收敛。<br>#年度关键词： 大模型时代来临之后，这个小方向也有开始往预训练、提示学习、强化学习、reward model转的趋势。</p>
<h1 id="稍作分类"><a href="#稍作分类" class="headerlink" title="稍作分类"></a>稍作分类</h1><p>两条路：处理成预测任务[1]；处理成匹配任务[2]。大多数文章都follow了[1]，也有少数沿着[2]的路线的文章如[5][22]，也有融合这两条路线的[33]。<br>跨模态视频时刻相关任务主要需要集中解决的问题有两点：一.跨模态 二.高效检索&#x2F;定位</p>
<p>对于一.</p>
<p>模态特征的细化：模态间的交互和融合[3-9，37，39，42]，局部对齐[17-19]，模态表征细粒度[20，21，28]，视觉目标间的关系[46]<br>结合其他外部信息，如字幕[35，36]<br>对于二.<br>检索方法主要涉及到候选集的问题</p>
<p>常规方法会提前切分好，或者修剪候选集[33]<br>生成候选：预测一些可能的边界[10]<br>探讨候选之间的关系：使用图或者矩阵的形式[13，26]<br>正负例样本不平衡问题[45]<br>定位方法可分直接预测和强化学习  </p>
<p>直接预测起止点：[11，31，34，36，38]，对边界预测的多任务增强如边界感知[24]，moment segmentation[41]。<br>强化学习：[12，14，15，25，29]，主要涉及一些语义概念融合，如何使强化学习更高效。<br>也有融合定位和检索的方法[30]，和其他任务结合的方法[50,60]  </p>
<p>除了一和二问题的，其他话题</p>
<p>标注text-video对太耗时，无监督方法[16，23，27，32]，主要会使用注意力对齐，正负样本对抗，多示例学习等等技术。<br>标注鲁棒性[34，40，41，48，49]，解决方案主要有比较分布和标注预测，因果推理，去偏。<br>收集数据集太耗时[62,66]，也是转向于无监督或者少样本学习。<br>搜索全库视频时刻[35，38]，要求对模态内和模态外都有更好的理解和区分能力。  </p>
<p><strong>感谢看到这里，在记录中收获成长，道阻且长</strong></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>mobbu
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://mobbu.space/2023/10/07/20231007_paperRead/" title="多模态视频片段定位论文阅读">http://mobbu.space/2023/10/07/20231007_paperRead/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/" rel="tag"># 多模态</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag"># 论文阅读</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/10/04/20231004_Latex&beamer/" rel="prev" title="学术PPT制作--beamer">
      <i class="fa fa-chevron-left"></i> 学术PPT制作--beamer
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/10/09/20231009_TipsOnLinux/" rel="next" title="Linux的操作记录">
      Linux的操作记录 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#2017-2"><span class="nav-number">1.</span> <span class="nav-text">2017:2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2018-5"><span class="nav-number">2.</span> <span class="nav-text">2018:5</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2019-16"><span class="nav-number">3.</span> <span class="nav-text">2019: 16</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2020-22"><span class="nav-number">4.</span> <span class="nav-text">2020:22</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2021-11"><span class="nav-number">5.</span> <span class="nav-text">2021:11</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2022-19"><span class="nav-number">6.</span> <span class="nav-text">2022:19</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2023-x"><span class="nav-number">7.</span> <span class="nav-text">2023:x</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A8%8D%E4%BD%9C%E5%88%86%E7%B1%BB"><span class="nav-number">8.</span> <span class="nav-text">稍作分类</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="mobbu"
      src="/images/pink_mob.jpeg">
  <p class="site-author-name" itemprop="name">mobbu</p>
  <div class="site-description" itemprop="description">道阻且长</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">41</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/mobbu919" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;mobbu919" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/mobbu777@163.com" title="E-Mail → mobbu777@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links Website
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.cnblogs.com/mobbu" title="https:&#x2F;&#x2F;www.cnblogs.com&#x2F;mobbu" rel="noopener" target="_blank">mobbu的博客园</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-08 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">mobbu</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">100k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:31</span>
</div>
  <div class="powered-by">
   <!--由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动-->
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '574a9ec89d2c91a007ad',
      clientSecret: 'fd02cd7c15478f08e869ea62cb030da32e88a10c',
      repo        : 'mobbu919.github.io',
      owner       : 'mobbu919',
      admin       : ['mobbu919'],
      id          : '57f6beffc7da3b96eebe8a2c3e78f2a4',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
